{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a8513c-6164-4b68-bad8-6a36c2678003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "try:\n",
    "    script_dir = Path(__FILE__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path('').resolve()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f553e16-1c77-4bc0-a7b1-01716f99df7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SQLite3Wrapper:\n",
    "    def __init__(self, path='database.db'):\n",
    "        self.connection = sqlite3.connect(path)\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def table_exists(self, table_name):\n",
    "        result = self.cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\")\n",
    "        return result.fetchone() != None\n",
    "    \n",
    "    def execute(self, *args, **kwargs):\n",
    "        return self.cursor.execute(*args, **kwargs)\n",
    "    \n",
    "    def print(self, *args, **kwargs):\n",
    "        print(*args)\n",
    "        print(kwargs)\n",
    "\n",
    "    def ensure_create(self, table_name, attributes):\n",
    "        if not self.table_exists(table_name):\n",
    "            print('Creating table', table_name, 'with attributes', attributes)\n",
    "            self.execute(f'CREATE TABLE {table_name}({attributes})')\n",
    "    \n",
    "    def count(self, table_name):\n",
    "        return self.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]\n",
    "    \n",
    "    def select(self, table_name):\n",
    "        return self.execute(f'SELECT * FROM {table_name}').fetchall()\n",
    "    \n",
    "    def get(self, query, params=[], default=None):\n",
    "        result = self.execute(query, params)\n",
    "        result = result.fetchone()\n",
    "        if result is None:\n",
    "            return default\n",
    "        else:\n",
    "            return result[0]\n",
    "    \n",
    "    def print(self, table_name):\n",
    "        for row in self.execute(f'SELECT * FROM {table_name}').fetchall():\n",
    "            print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bd9036-2043-4b37-a71b-73fb8a11e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(w):\n",
    "    print('Loading raw data from csv file.')\n",
    "    data = pd.read_csv('sources/dataset/Gungor_2018_VictorianAuthorAttribution_data-train.csv', encoding='latin-1')\n",
    "    print('Putting raw data into sqlite table.')\n",
    "    for index, row in data.iterrows():\n",
    "        w.execute('INSERT INTO data VALUES (?, ?)', (row['text'], row['author']))\n",
    "        print(row['author'])\n",
    "    w.connection.commit()\n",
    "    print('sqlite data table ready.')\n",
    "\n",
    "def initialize_word_counts(w):\n",
    "    # Not sure why I bothered making this.\n",
    "    print('Counting words.')\n",
    "    # w.execute('DELETE FROM word_counts')\n",
    "    rows = w.execute('SELECT * FROM data').fetchall()\n",
    "    for row in rows:\n",
    "        text, author = row\n",
    "        print('Author ', author, end='\\r')\n",
    "        for word in text.rstrip(' ').split(' '):\n",
    "            old_count = w.execute('SELECT count FROM word_counts WHERE word = ?', (word,)).fetchone()\n",
    "            if old_count is None:\n",
    "                w.execute('INSERT INTO word_counts (word, count) VALUES (?, ?)', (word, 1))\n",
    "            else:\n",
    "                old_count ,= old_count\n",
    "                w.execute('UPDATE word_counts SET count = ? WHERE word = ?', (old_count + 1, word))\n",
    "        \n",
    "    w.connection.commit()\n",
    "\n",
    "    w.print('word_counts')\n",
    "    print(w.count('word_counts'))\n",
    "\n",
    "def initialize_id_word(w):\n",
    "    rows = w.execute('SELECT * FROM data').fetchall()\n",
    "    for row in rows:\n",
    "        text, author = row\n",
    "        print('Author ', author, end='\\r')\n",
    "        for word in text.rstrip(' ').split(' '):\n",
    "            w.execute('INSERT OR IGNORE INTO id_word (word) VALUES (?)', [word])\n",
    "    print()\n",
    "    \n",
    "    w.print('id_word')\n",
    "    w.connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b754d2-0114-4f10-a383-a54d6f67d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rows into Python list\n",
      "Author: 5014 1822 26 41\n",
      "Done loading rows\n",
      "Loading into dataframe\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "w = SQLite3Wrapper()\n",
    "w.ensure_create('data', 'words TEXT, author INT')\n",
    "\n",
    "# w.ensure_create('word_counts', 'word VARCHAR(255), count INT')\n",
    "# w.execute('CREATE UNIQUE INDEX IF NOT EXISTS index_word ON word_counts (word)')\n",
    "\n",
    "# w.execute('DROP TABLE IF EXISTS id_word')\n",
    "w.ensure_create('id_word', 'id INTEGER PRIMARY KEY, word VARCHAR(255) UNIQUE')\n",
    "w.execute('CREATE UNIQUE INDEX IF NOT EXISTS index_word ON id_word (word)')\n",
    "\n",
    "if w.count('data') < 53678: initialize_data(w)\n",
    "if False: initialize_word_counts(w)\n",
    "if w.count('id_word') < 10000: initialize_id_word(w)\n",
    "\n",
    "word_to_id = {word:id for id, word in w.select('id_word')}\n",
    "id_to_word = [None] + list(sorted((word for word, id in word_to_id.items()), key=lambda word : word_to_id[word]))\n",
    "\n",
    "id_to_author = [None, 'Arthur Conan Doyle', 'Charles Darwin', 'Charles Dickens', 'Edith Wharton', 'George Eliot', 'Horace Greeley', 'Jack London', 'James Baldwin', 'Jane Austen', 'John Muir', 'Joseph Conrad', 'Mark Twain', 'Nathaniel Hawthorne', 'Ralph Emerson', 'Robert Louis Stevenson', 'Rudyard Kipling', 'Sinclair Lewis', 'Theodore Dreiser', 'Thomas Hardy', 'Walt Whitman', 'Washington Irving', 'William Carleton', 'Albert Ross', 'Anne Manning', 'Arlo Bates', 'Bret Harte', 'Catharine Maria Sedgwick', 'Charles Reade', 'Edward Eggleston', 'Fergus Hume', 'Frances Hodgson Burnett', 'George Moore', 'George William Curtis', 'Helen Mathers', 'Henry Rider Haggard', 'Isabella Lucy Bird', 'Jacob Abbott', 'James Grant', 'James Payn', 'John Kendrick Bangs', 'John Pendleton Kennedy', 'John Strange Winter', 'Lucas Malet', 'Marie Corelli', 'Oliver Optic', 'Sarah Orne Jewett', 'Sarah Stickney Ellis', 'Thomas Anstey Guthrie', 'Thomas Nelson Page', 'William Black']\n",
    "author_to_id = {author:id for id, author in enumerate(id_to_author)}\n",
    "\n",
    "rows = []\n",
    "print('Loading rows into Python list')\n",
    "for text, author in w.select('data'):\n",
    "    words = text.rstrip(' ').split(' ')\n",
    "    row = [word_to_id[word] for word in words] + [author]\n",
    "    rows.append(row)\n",
    "    print('Author:', author, end='\\r')\n",
    "print('\\nDone loading rows')\n",
    "\n",
    "print('Loading into dataframe')\n",
    "data = pd.DataFrame(rows, columns=list(range(1000)) + ['author'])\n",
    "print('Ok')\n",
    "\n",
    "valid_authors = {id_author[0]:id_author[1] for id_author in enumerate(id_to_author) if id_author != None and id_author[0] in data['author'].values}\n",
    "train_data, test_data = train_test_split(data, train_size=0.8, random_state=6128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f31151-1350-4270-b2c1-f40a803a0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_row_to_words(pandas_row):\n",
    "    index = sorted(i for i in pandas_row.index if isinstance(i, int))\n",
    "    return ' '.join(id_to_word[pandas_row[word_index]] for word_index in index)\n",
    "\n",
    "def get_author_rows(author_id):\n",
    "    return data[data['author'] == author_id]\n",
    "\n",
    "author_tensors = []\n",
    "def get_author_tensor(author_id):\n",
    "    while author_id >= len(author_tensors):\n",
    "        rows = get_author_rows(len(author_tensors)).drop(['author'], axis=1)\n",
    "        author_tensors.append(torch.tensor(rows.values, dtype=torch.int64))\n",
    "    return author_tensors[author_id]\n",
    "\n",
    "def tensor_to_words(t):\n",
    "    if len(t.shape) == 0: return id_to_word[t.tolist()]\n",
    "    if len(t.shape) == 1: return ' '.join(id_to_word[i] for i in t.tolist())\n",
    "    return '\\n'.join(tensor_to_words(subtensor) for subtensor in t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9afe3f2e-91ca-4eb7-826f-98ac6403097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 24 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 160000\n",
    "eval_interval = 20000\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.3\n",
    "\n",
    "torch.manual_seed(2965)\n",
    "print('device is', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cd5b13-23aa-4ddb-9649-984441ddc887",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(author_id=12):\n",
    "    # Select random excerpt by the author\n",
    "    #  then select random start point within the excerpt.\n",
    "    data = get_author_tensor(author_id)\n",
    "    selected_excerpts = torch.randint(data.shape[0], (batch_size,))\n",
    "    selected_starts = torch.randint(data.shape[1] - block_size, (batch_size,))\n",
    "    excerpt_start = torch.stack((selected_excerpts, selected_starts), axis=1)\n",
    "    x = torch.stack([data[e][s:s+block_size] for e, s in excerpt_start]).cuda()\n",
    "    y = torch.stack([data[e][s+1:s+block_size+1] for e, s in excerpt_start]).cuda()\n",
    "    return x, y\n",
    "\n",
    "# xb, yb = get_batch()\n",
    "# print('inputs:')\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print('targets:')\n",
    "# print(yb.shape)\n",
    "# print(yb)\n",
    "# for b in range(batch_size): # batch dimension\n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {repr(tensor_to_words(context))} the target is {repr(tensor_to_words(target))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba11d3d7-1b19-4460-9046-e8752667bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, author):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(author)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aeb38bb-3f2a-46e3-9eb5-c25afcff749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae864bcb-0ee9-4546-b9db-742d2672359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there's a 'model.pth' in the directory before training the model\n",
    "def load_or_train(author):\n",
    "    model = BigramLanguageModel(vocab_size=len(id_to_word))\n",
    "    m = model.to(device)\n",
    "\n",
    "    models_dir = script_dir.joinpath('models')\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    expected_path = models_dir.joinpath(f'model{author}.pth')\n",
    "    if expected_path.exists():\n",
    "        print(f\"Loading model {author} '{id_to_author[author]}'...\")\n",
    "        model.load_state_dict(torch.load(expected_path))\n",
    "    else:\n",
    "        # create a PyTorch optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training model for author {author} '{id_to_author[author]}',  {sum(p.numel() for p in m.parameters())/1e6:.2f} M params, {max_iters} iterations...\")\n",
    "        for iter in range(max_iters + 1):\n",
    "            print('Iteration', iter, '           ', end='\\r')\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if iter % eval_interval == 0:\n",
    "                loss = estimate_loss(model, author)\n",
    "                print()\n",
    "                print(f\"step {iter}: loss {loss:.4f}\")\n",
    "        \n",
    "            xb, yb = get_batch(author)\n",
    "        \n",
    "            # evaluate the loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # save the model\n",
    "        print('Saving model')\n",
    "        torch.save(model.state_dict(), expected_path)\n",
    "    print(\"Ok\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "575d5512-f714-4953-815a-3cbc3ca058d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for author 1 'Arthur Conan Doyle',  1.70 M params, 160000 iterations...\n",
      "Iteration 0            \n",
      "step 0: loss 9.3648\n",
      "Iteration 20000            \n",
      "step 20000: loss 4.2312\n",
      "Iteration 40000            \n",
      "step 40000: loss 3.9808\n",
      "Iteration 60000            \n",
      "step 60000: loss 3.8338\n",
      "Iteration 80000            \n",
      "step 80000: loss 3.7388\n",
      "Iteration 100000            \n",
      "step 100000: loss 3.6836\n",
      "Iteration 120000            \n",
      "step 120000: loss 3.6456\n",
      "Iteration 140000            \n",
      "step 140000: loss 3.6068\n",
      "Iteration 160000            \n",
      "step 160000: loss 3.5883\n",
      "Saving model\n",
      "Ok\n",
      "Training model for author 2 'Charles Darwin',  1.70 M params, 160000 iterations...\n",
      "Iteration 0            \n",
      "step 0: loss 9.3652\n",
      "Iteration 20000            \n",
      "step 20000: loss 3.8359\n",
      "Iteration 40000            \n",
      "step 40000: loss 3.3376\n",
      "Iteration 60000            \n",
      "step 60000: loss 3.0668\n",
      "Iteration 80000            \n",
      "step 80000: loss 2.9082\n",
      "Iteration 100000            \n",
      "step 100000: loss 2.8127\n",
      "Iteration 120000            \n",
      "step 120000: loss 2.7239\n",
      "Iteration 140000            \n",
      "step 140000: loss 2.6696\n",
      "Iteration 160000            \n",
      "step 160000: loss 2.6232\n",
      "Saving model\n",
      "Ok\n",
      "Training model for author 3 'Charles Dickens',  1.70 M params, 160000 iterations...\n",
      "Iteration 0            \n",
      "step 0: loss 9.3663\n",
      "Iteration 20000            \n",
      "step 20000: loss 2.7523\n",
      "Iteration 40000            \n",
      "step 40000: loss 2.1658\n",
      "Iteration 60000            \n",
      "step 60000: loss 1.8519\n",
      "Iteration 80000            \n",
      "step 80000: loss 1.6932\n",
      "Iteration 100000            \n",
      "step 100000: loss 1.5850\n",
      "Iteration 120000            \n",
      "step 120000: loss 1.5068\n",
      "Iteration 140000            \n",
      "step 140000: loss 1.4406\n",
      "Iteration 160000            \n",
      "step 160000: loss 1.4062\n",
      "Saving model\n",
      "Ok\n",
      "Training model for author 4 'Edith Wharton',  1.70 M params, 160000 iterations...\n",
      "Iteration 0            \n",
      "step 0: loss 9.3812\n",
      "Iteration 20000            \n",
      "step 20000: loss 4.5412\n",
      "Iteration 40000            \n",
      "step 40000: loss 4.3403\n",
      "Iteration 60000            \n",
      "step 60000: loss 4.2186\n",
      "Iteration 80000            \n",
      "step 80000: loss 4.1358\n",
      "Iteration 100000            \n",
      "step 100000: loss 4.1024\n",
      "Iteration 120000            \n",
      "step 120000: loss 4.0635\n",
      "Iteration 140000            \n",
      "step 140000: loss 4.0299\n",
      "Iteration 160000            \n",
      "step 160000: loss 4.0125\n",
      "Saving model\n",
      "Ok\n",
      "Training model for author 6 'Horace Greeley',  1.70 M params, 160000 iterations...\n",
      "Iteration 0            \n",
      "step 0: loss 9.4104\n",
      "Iteration 1447            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author_id \u001b[38;5;129;01min\u001b[39;00m valid_authors:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mload_or_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m, in \u001b[0;36mload_or_train\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m     24\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(author)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 91\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     89\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 37\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 37\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(wei)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# perform the weighted aggregation of the values\u001b[39;00m\n\u001b[1;32m     23\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/life_management/college/machine_learning/h6/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for author_id in valid_authors:\n",
    "    load_or_train(author_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef8589c-c021-4dcd-a7dd-85ec74fca4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model {author} '{id_to_author[author]}'...\n",
      "Ok\n",
      "he but his would make himself disagreeable what a well you are going to do said mrs with a lunatic a direct in the i have been mad said mrs when my mother was out of the house that lady seems to know i don t but she said nothing at one time had i served this â a â came on the tree and found her placed it over by the dagger on the mused and see ct io it must have been from the author of the sea ware believed that he had come here safe without in this somewhat the lost she did not wish to turn her attention as could forbear a spirit of her own she now and deprived of him to confide his chance words which might wish to the of them in he would have take it secretly from a and let me about it on a stick a little into that is missing and if any other man and the less has been related in all not likely to afford the chance of his descent to the royal cause but only were exact in his possession of s will he wished to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate from the model\n",
    "def generate_sample(model):\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    idx = model.generate(context, max_new_tokens=200)\n",
    "    idx_list = idx[0][1:]\n",
    "    return tensor_to_words(idx_list)\n",
    "print(generate_sample(load_or_train(12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3125bb88-c1d8-401d-9d58-85b386588411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model {author} '{id_to_author[author]}'...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "working_model = load_or_train(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9640973-9240-435e-bb0a-0299e0ca10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probabilities(model, tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t_sample_prefix_logits, _ = model(tensor.unsqueeze(0).to(device))\n",
    "    t_prefix_logits = t_sample_prefix_logits[0]\n",
    "    t_logits = t_prefix_logits[-1]\n",
    "    t_probabilities = F.softmax(t_logits, dim=0)\n",
    "    return t_probabilities\n",
    "\n",
    "def predict_word_ids(model, tensor):\n",
    "    t_probabilities = predict_probabilities(model, tensor)\n",
    "    top_word_things = torch.topk(t_probabilities, 5)\n",
    "\n",
    "    best = []\n",
    "    cumulative_probability = 0\n",
    "    for word_id, word_probability in zip(top_word_things.indices, top_word_things.values):\n",
    "        word_id = word_id.tolist()\n",
    "        word_probability = word_probability.tolist()\n",
    "        best.append((word_id, word_probability))\n",
    "        cumulative_probability += word_probability\n",
    "        if cumulative_probability >= 0.95:\n",
    "            break\n",
    "    \n",
    "    return best\n",
    "\n",
    "def predict_words(model, tensor):\n",
    "    word_ids = predict_word_ids(model, tensor)\n",
    "    words = [id_to_word[word_id] for word_id, probability in word_ids]\n",
    "    return words\n",
    "\n",
    "working_tensor = get_author_tensor(12)[10][:45]\n",
    "print('Seeing',\n",
    "      repr(tensor_to_words(working_tensor[:-1])),\n",
    "      '\\nwe expect',\n",
    "      predict_words(working_model, working_tensor[:-1]),\n",
    "      'when the real value is',\n",
    "      tensor_to_words(working_tensor[-1])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776af61-10f6-4578-83c0-9835cd3f4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the probability averaged over the input text\n",
    "def get_prob_avg(model, tensor):\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(1, tensor.shape[0] - block_size):\n",
    "        print(i)\n",
    "    return 12\n",
    "\n",
    "    # Total number of chunks\n",
    "    total_chunks = len(input_text) // block_size\n",
    "    if len(input_text) % block_size != 0:\n",
    "        total_chunks += 1  # Account for the last shorter chunk if any\n",
    "\n",
    "    avg_probs = []\n",
    "\n",
    "    # Process each chunk\n",
    "    for i in range(total_chunks):\n",
    "        start_index = i * block_size\n",
    "        end_index = start_index + block_size\n",
    "        chunk = input_text[start_index:end_index]\n",
    "\n",
    "        # Encode the chunk\n",
    "        encoded_chunk = torch.tensor(encode(chunk), dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "        # Run the model on the chunk\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(encoded_chunk)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # Apply softmax over the last dimension (vocab_size)\n",
    "\n",
    "        # Get top-k probabilities for the last token of the chunk\n",
    "        last_token_probs = probs[0, -1, :]  # Probabilities of the last token \n",
    "        # top_k_probs, _ = torch.topk(last_token_probs, k)\n",
    "\n",
    "        # Calculate average of top-k probabilities and store\n",
    "        # avg_prob = torch.mean(top_k_probs).item()\n",
    "        avg_probs.append(last_token_probs.max().item())\n",
    "\n",
    "    # Calculate the overall average probability across all chunks\n",
    "    overall_avg_prob = sum(avg_probs) / len(avg_probs)\n",
    "    return overall_avg_prob\n",
    "\n",
    "print(f\"Average probability of author 12 being author {12}: {get_prob_avg(working_model, get_author_tensor(12)[10]):.4f}\")\n",
    "# print(f\"Average probability of author 17 being author {12}: {get_prob_avg(working_model, get_author_tensor(17)[10]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc2dd4-7aa8-4771-91dd-0b3129ff825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Average probability of author 12 being author {12}: {get_prob_avg(working_model, get_author_tensor(12)[10]):.4f}\")\n",
    "print(f\"Average probability of author 17 being author {12}: {get_prob_avg(working_model, get_author_tensor(17)[10]):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
