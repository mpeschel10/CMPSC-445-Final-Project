{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a8513c-6164-4b68-bad8-6a36c2678003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "try:\n",
    "    script_dir = Path(__FILE__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path('').resolve()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f553e16-1c77-4bc0-a7b1-01716f99df7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SQLite3Wrapper:\n",
    "    def __init__(self, path='database.db'):\n",
    "        self.connection = sqlite3.connect(path)\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def table_exists(self, table_name):\n",
    "        result = self.cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\")\n",
    "        return result.fetchone() != None\n",
    "    \n",
    "    def execute(self, *args, **kwargs):\n",
    "        return self.cursor.execute(*args, **kwargs)\n",
    "    \n",
    "    def print(self, *args, **kwargs):\n",
    "        print(*args)\n",
    "        print(kwargs)\n",
    "\n",
    "    def ensure_create(self, table_name, attributes):\n",
    "        if not self.table_exists(table_name):\n",
    "            print('Creating table', table_name, 'with attributes', attributes)\n",
    "            self.execute(f'CREATE TABLE {table_name}({attributes})')\n",
    "    \n",
    "    def count(self, table_name):\n",
    "        return self.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]\n",
    "    \n",
    "    def select(self, table_name):\n",
    "        return self.execute(f'SELECT * FROM {table_name}').fetchall()\n",
    "    \n",
    "    def get(self, query, params=[], default=None):\n",
    "        result = self.execute(query, params)\n",
    "        result = result.fetchone()\n",
    "        if result is None:\n",
    "            return default\n",
    "        else:\n",
    "            return result[0]\n",
    "    \n",
    "    def print(self, table_name):\n",
    "        for row in self.execute(f'SELECT * FROM {table_name}').fetchall():\n",
    "            print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bd9036-2043-4b37-a71b-73fb8a11e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(w):\n",
    "    print('Loading raw data from csv file.')\n",
    "    data = pd.read_csv('sources/dataset/Gungor_2018_VictorianAuthorAttribution_data-train.csv', encoding='latin-1')\n",
    "    print('Putting raw data into sqlite table.')\n",
    "    for index, row in data.iterrows():\n",
    "        w.execute('INSERT INTO data VALUES (?, ?)', (row['text'], row['author']))\n",
    "        print(row['author'])\n",
    "    w.connection.commit()\n",
    "    print('sqlite data table ready.')\n",
    "\n",
    "def initialize_word_counts(w):\n",
    "    # Not sure why I bothered making this.\n",
    "    print('Counting words.')\n",
    "    # w.execute('DELETE FROM word_counts')\n",
    "    rows = w.execute('SELECT * FROM data').fetchall()\n",
    "    for row in rows:\n",
    "        text, author = row\n",
    "        print('Author ', author, end='\\r')\n",
    "        for word in text.rstrip(' ').split(' '):\n",
    "            old_count = w.execute('SELECT count FROM word_counts WHERE word = ?', (word,)).fetchone()\n",
    "            if old_count is None:\n",
    "                w.execute('INSERT INTO word_counts (word, count) VALUES (?, ?)', (word, 1))\n",
    "            else:\n",
    "                old_count ,= old_count\n",
    "                w.execute('UPDATE word_counts SET count = ? WHERE word = ?', (old_count + 1, word))\n",
    "        \n",
    "    w.connection.commit()\n",
    "\n",
    "    w.print('word_counts')\n",
    "    print(w.count('word_counts'))\n",
    "\n",
    "def initialize_id_word(w):\n",
    "    rows = w.execute('SELECT * FROM data').fetchall()\n",
    "    for row in rows:\n",
    "        text, author = row\n",
    "        print('Author ', author, end='\\r')\n",
    "        for word in text.rstrip(' ').split(' '):\n",
    "            w.execute('INSERT OR IGNORE INTO id_word (word) VALUES (?)', [word])\n",
    "    print()\n",
    "    \n",
    "    w.print('id_word')\n",
    "    w.connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b754d2-0114-4f10-a383-a54d6f67d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rows into Python list\n",
      "Author: 50 2630 41 48\n",
      "Done loading rows\n",
      "Loading into dataframe\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "w = SQLite3Wrapper()\n",
    "w.ensure_create('data', 'words TEXT, author INT')\n",
    "\n",
    "# w.ensure_create('word_counts', 'word VARCHAR(255), count INT')\n",
    "# w.execute('CREATE UNIQUE INDEX IF NOT EXISTS index_word ON word_counts (word)')\n",
    "\n",
    "# w.execute('DROP TABLE IF EXISTS id_word')\n",
    "w.ensure_create('id_word', 'id INTEGER PRIMARY KEY, word VARCHAR(255) UNIQUE')\n",
    "w.execute('CREATE UNIQUE INDEX IF NOT EXISTS index_word ON id_word (word)')\n",
    "\n",
    "if w.count('data') < 53678: initialize_data(w)\n",
    "if False: initialize_word_counts(w)\n",
    "if w.count('id_word') < 10000: initialize_id_word(w)\n",
    "\n",
    "word_to_id = {word:id for id, word in w.select('id_word')}\n",
    "id_to_word = [None] + list(sorted((word for word, id in word_to_id.items()), key=lambda word : word_to_id[word]))\n",
    "\n",
    "id_to_author = [None, 'Arthur Conan Doyle', 'Charles Darwin', 'Charles Dickens', 'Edith Wharton', 'George Eliot', 'Horace Greeley', 'Jack London', 'James Baldwin', 'Jane Austen', 'John Muir', 'Joseph Conrad', 'Mark Twain', 'Nathaniel Hawthorne', 'Ralph Emerson', 'Robert Louis Stevenson', 'Rudyard Kipling', 'Sinclair Lewis', 'Theodore Dreiser', 'Thomas Hardy', 'Walt Whitman', 'Washington Irving', 'William Carleton', 'Albert Ross', 'Anne Manning', 'Arlo Bates', 'Bret Harte', 'Catharine Maria Sedgwick', 'Charles Reade', 'Edward Eggleston', 'Fergus Hume', 'Frances Hodgson Burnett', 'George Moore', 'George William Curtis', 'Helen Mathers', 'Henry Rider Haggard', 'Isabella Lucy Bird', 'Jacob Abbott', 'James Grant', 'James Payn', 'John Kendrick Bangs', 'John Pendleton Kennedy', 'John Strange Winter', 'Lucas Malet', 'Marie Corelli', 'Oliver Optic', 'Sarah Orne Jewett', 'Sarah Stickney Ellis', 'Thomas Anstey Guthrie', 'Thomas Nelson Page', 'William Black']\n",
    "author_to_id = {author:id for id, author in enumerate(id_to_author)}\n",
    "\n",
    "rows = []\n",
    "print('Loading rows into Python list')\n",
    "for text, author in w.select('data'):\n",
    "    words = text.rstrip(' ').split(' ')\n",
    "    row = [word_to_id[word] for word in words] + [author]\n",
    "    rows.append(row)\n",
    "    print('Author:', author, end='\\r')\n",
    "print('\\nDone loading rows')\n",
    "\n",
    "print('Loading into dataframe')\n",
    "data = pd.DataFrame(rows, columns=list(range(1000)) + ['author'])\n",
    "print('Ok')\n",
    "\n",
    "valid_authors = {id_author[0]:id_author[1] for id_author in enumerate(id_to_author) if id_author != None and id_author[0] in data['author'].values}\n",
    "train_data, test_data = train_test_split(data, train_size=0.8, random_state=6128)\n",
    "test_data = torch.tensor(test_data.values, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f31151-1350-4270-b2c1-f40a803a0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_row_to_words(pandas_row):\n",
    "    index = sorted(i for i in pandas_row.index if isinstance(i, int))\n",
    "    return ' '.join(id_to_word[pandas_row[word_index]] for word_index in index)\n",
    "\n",
    "def get_author_rows(author_id):\n",
    "    return data[data['author'] == author_id]\n",
    "\n",
    "author_tensors = []\n",
    "def get_author_tensor(author_id):\n",
    "    while author_id >= len(author_tensors):\n",
    "        rows = get_author_rows(len(author_tensors)).drop(['author'], axis=1)\n",
    "        author_tensors.append(torch.tensor(rows.values, dtype=torch.int64))\n",
    "    return author_tensors[author_id]\n",
    "\n",
    "def tensor_to_words(t):\n",
    "    if len(t.shape) == 0: return id_to_word[t.tolist()]\n",
    "    if len(t.shape) == 1: return ' '.join(id_to_word[i] for i in t.tolist())\n",
    "    return '\\n'.join(tensor_to_words(subtensor) for subtensor in t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afe3f2e-91ca-4eb7-826f-98ac6403097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 24 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 160000\n",
    "eval_interval = 20000\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.3\n",
    "\n",
    "torch.manual_seed(2965)\n",
    "print('device is', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cd5b13-23aa-4ddb-9649-984441ddc887",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(author_id=12):\n",
    "    # Select random excerpt by the author\n",
    "    #  then select random start point within the excerpt.\n",
    "    data = get_author_tensor(author_id)\n",
    "    selected_excerpts = torch.randint(data.shape[0], (batch_size,))\n",
    "    selected_starts = torch.randint(data.shape[1] - block_size, (batch_size,))\n",
    "    excerpt_start = torch.stack((selected_excerpts, selected_starts), axis=1)\n",
    "    x = torch.stack([data[e][s:s+block_size] for e, s in excerpt_start]).cuda()\n",
    "    y = torch.stack([data[e][s+1:s+block_size+1] for e, s in excerpt_start]).cuda()\n",
    "    return x, y\n",
    "\n",
    "# xb, yb = get_batch()\n",
    "# print('inputs:')\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print('targets:')\n",
    "# print(yb.shape)\n",
    "# print(yb)\n",
    "# for b in range(batch_size): # batch dimension\n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {repr(tensor_to_words(context))} the target is {repr(tensor_to_words(target))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba11d3d7-1b19-4460-9046-e8752667bbc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, author):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(author)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aeb38bb-3f2a-46e3-9eb5-c25afcff749c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae864bcb-0ee9-4546-b9db-742d2672359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there's a 'model.pth' in the directory before training the model\n",
    "def load_or_train(author):\n",
    "    model = BigramLanguageModel(vocab_size=len(id_to_word))\n",
    "    m = model.to(device)\n",
    "\n",
    "    models_dir = script_dir.joinpath('models')\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    expected_path = models_dir.joinpath(f'model{author}.pth')\n",
    "    if expected_path.exists():\n",
    "        print(f\"Loading model {author} '{id_to_author[author]}'...\")\n",
    "        model.load_state_dict(torch.load(expected_path))\n",
    "    else:\n",
    "        # create a PyTorch optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training model for author {author} '{id_to_author[author]}',  {sum(p.numel() for p in m.parameters())/1e6:.2f} M params, {max_iters} iterations...\")\n",
    "        for iter in range(max_iters + 1):\n",
    "            print('Iteration', iter, '           ', end='\\r')\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if iter % eval_interval == 0:\n",
    "                loss = estimate_loss(model, author)\n",
    "                print()\n",
    "                print(f\"step {iter}: loss {loss:.4f}\")\n",
    "        \n",
    "            xb, yb = get_batch(author)\n",
    "        \n",
    "            # evaluate the loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # save the model\n",
    "        print('Saving model')\n",
    "        torch.save(model.state_dict(), expected_path)\n",
    "    print(\"Ok\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575d5512-f714-4953-815a-3cbc3ca058d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1 'Arthur Conan Doyle'...\n",
      "Ok\n",
      "Loading model 2 'Charles Darwin'...\n",
      "Ok\n",
      "Loading model 3 'Charles Dickens'...\n",
      "Ok\n",
      "Loading model 4 'Edith Wharton'...\n",
      "Ok\n",
      "Loading model 6 'Horace Greeley'...\n",
      "Ok\n",
      "Loading model 8 'James Baldwin'...\n",
      "Ok\n",
      "Loading model 9 'Jane Austen'...\n",
      "Ok\n",
      "Loading model 10 'John Muir'...\n",
      "Ok\n",
      "Loading model 11 'Joseph Conrad'...\n",
      "Ok\n",
      "Loading model 12 'Mark Twain'...\n",
      "Ok\n",
      "Loading model 13 'Nathaniel Hawthorne'...\n",
      "Ok\n",
      "Loading model 14 'Ralph Emerson'...\n",
      "Ok\n",
      "Loading model 15 'Robert Louis Stevenson'...\n",
      "Ok\n",
      "Loading model 16 'Rudyard Kipling'...\n",
      "Ok\n",
      "Loading model 17 'Sinclair Lewis'...\n",
      "Ok\n",
      "Loading model 18 'Theodore Dreiser'...\n",
      "Ok\n",
      "Loading model 19 'Thomas Hardy'...\n",
      "Ok\n",
      "Loading model 20 'Walt Whitman'...\n",
      "Ok\n",
      "Loading model 21 'Washington Irving'...\n",
      "Ok\n",
      "Loading model 22 'William Carleton'...\n",
      "Ok\n",
      "Loading model 23 'Albert Ross'...\n",
      "Ok\n",
      "Loading model 24 'Anne Manning'...\n",
      "Ok\n",
      "Loading model 25 'Arlo Bates'...\n",
      "Ok\n",
      "Loading model 26 'Bret Harte'...\n",
      "Ok\n",
      "Loading model 27 'Catharine Maria Sedgwick'...\n",
      "Ok\n",
      "Loading model 28 'Charles Reade'...\n",
      "Ok\n",
      "Loading model 29 'Edward Eggleston'...\n",
      "Ok\n",
      "Loading model 30 'Fergus Hume'...\n",
      "Ok\n",
      "Loading model 32 'George Moore'...\n",
      "Ok\n",
      "Loading model 33 'George William Curtis'...\n",
      "Ok\n",
      "Loading model 34 'Helen Mathers'...\n",
      "Ok\n",
      "Loading model 35 'Henry Rider Haggard'...\n",
      "Ok\n",
      "Loading model 36 'Isabella Lucy Bird'...\n",
      "Ok\n",
      "Loading model 37 'Jacob Abbott'...\n",
      "Ok\n",
      "Loading model 38 'James Grant'...\n",
      "Ok\n",
      "Loading model 39 'James Payn'...\n",
      "Ok\n",
      "Loading model 40 'John Kendrick Bangs'...\n",
      "Ok\n",
      "Loading model 41 'John Pendleton Kennedy'...\n",
      "Ok\n",
      "Loading model 42 'John Strange Winter'...\n",
      "Ok\n",
      "Loading model 43 'Lucas Malet'...\n",
      "Ok\n",
      "Loading model 44 'Marie Corelli'...\n",
      "Ok\n",
      "Loading model 45 'Oliver Optic'...\n",
      "Ok\n",
      "Loading model 46 'Sarah Orne Jewett'...\n",
      "Ok\n",
      "Loading model 48 'Thomas Anstey Guthrie'...\n",
      "Ok\n",
      "Loading model 50 'William Black'...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "# valid_authors = {id: name for id, name in valid_authors.items() if id <= 40}\n",
    "# print(valid_authors)\n",
    "for author_id in valid_authors:\n",
    "    load_or_train(author_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef8589c-c021-4dcd-a7dd-85ec74fca4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 12 'Mark Twain'...\n",
      "Ok\n",
      "speaking of her i d leave england with a jolly are heaps than you concluded that she has been for a certain degree for getting over the fall of mrs clear says that i say you also or description like some other person jumped up which the evidence against her dark and back on the contrary she looked well arrayed in nervous at the rusty manner do you know your errand sir that you stole it what do you mean man intend to do answered coolly he had a heart doctor if your thought cried with a smile pardon you have already been lost so far as they are really simple craft now then that s brain is all round my now false do you think that altered the papers i wish to wish to the bush and the he would have tried to trouble from a speech which was about to find its secret the man might be sure well though if any other gifts were strangers i left by the suggestion of the old man who was his servant and to doesn t replied readily lest he should kill the negro showing don t be arrested he was\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate from the model\n",
    "def generate_sample(model):\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    idx = model.generate(context, max_new_tokens=200)\n",
    "    idx_list = idx[0][1:]\n",
    "    return tensor_to_words(idx_list)\n",
    "print(generate_sample(load_or_train(12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3125bb88-c1d8-401d-9d58-85b386588411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 12 'Mark Twain'...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "working_model = load_or_train(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9640973-9240-435e-bb0a-0299e0ca10b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeing 'few days it is a forlorn' \n",
      "we expect ['hope', 'interest', 'amount', 'for', 'least'] when the real value is hope\n"
     ]
    }
   ],
   "source": [
    "def predict_probabilities(model, tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t_sample_prefix_logits, _ = model(tensor.unsqueeze(0))\n",
    "    t_sample_prefix_logits = t_sample_prefix_logits\n",
    "    t_prefix_logits = t_sample_prefix_logits[0]\n",
    "    t_logits = t_prefix_logits[-1]\n",
    "    return F.softmax(t_logits, dim=0)\n",
    "\n",
    "def predict_probabilities2(model, tensor):\n",
    "    # print('Evaluating model')\n",
    "    with torch.no_grad():\n",
    "        t_sample_prefix_logits, _ = model(tensor)\n",
    "    # print('Doing softmax etc')\n",
    "    t_sample_logits = t_sample_prefix_logits[:, t_sample_prefix_logits.shape[1] - 1, :]\n",
    "    return F.softmax(t_sample_logits, dim=1)\n",
    "\n",
    "def predict_word_ids(model, tensor):\n",
    "    t_probabilities = predict_probabilities(model, tensor)\n",
    "    top_word_things = torch.topk(t_probabilities, 5)\n",
    "\n",
    "    best = []\n",
    "    cumulative_probability = 0\n",
    "    for word_id, word_probability in zip(top_word_things.indices, top_word_things.values):\n",
    "        word_id = word_id.tolist()\n",
    "        word_probability = word_probability.tolist()\n",
    "        best.append((word_id, word_probability))\n",
    "        cumulative_probability += word_probability\n",
    "        if cumulative_probability >= 0.95:\n",
    "            break\n",
    "    \n",
    "    return best\n",
    "\n",
    "def predict_words(model, tensor):\n",
    "    word_ids = predict_word_ids(model, tensor)\n",
    "    words = [id_to_word[word_id] for word_id, probability in word_ids]\n",
    "    return words\n",
    "\n",
    "working_tensor = get_author_tensor(12)[10][:7]\n",
    "working_tensor = working_tensor.to(device)\n",
    "print('Seeing',\n",
    "      repr(tensor_to_words(working_tensor[:-1])),\n",
    "      '\\nwe expect',\n",
    "      predict_words(working_model, working_tensor[:-1]),\n",
    "      'when the real value is',\n",
    "      tensor_to_words(working_tensor[-1])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b776af61-10f6-4578-83c0-9835cd3f4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 12 'Mark Twain'...\n",
      "Ok\n",
      "Average probability of author 12 being author 12: tensor([0.1262], device='cuda:0')\n",
      "Time diff:  9.683106660842896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the probability averaged over the input text\n",
    "def get_prob_avg(model, tensor):\n",
    "    tensor = tensor.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    so_far = 0\n",
    "    for i in range(block_size, tensor.shape[0] - block_size):\n",
    "        start_point = i - block_size\n",
    "        # if start_point < 0: start_point = 0\n",
    "        probabilities = predict_probabilities(model, tensor[start_point:i])\n",
    "        expected_id = tensor[i]\n",
    "        context_start = i - 10;\n",
    "        if context_start < 0: context_start = 0\n",
    "        # print('Context:', tensor_to_words(tensor[context_start:i]), 'Expected:', tensor_to_words(expected_id), f'Observed: {probabilities[expected_id] * 100:.0f}%')\n",
    "        so_far += probabilities[expected_id]\n",
    "    return so_far / tensor.shape[0]\n",
    "\n",
    "def get_prob_avg2(model, tensor):\n",
    "    tensor = tensor.to(device)\n",
    "    take_offsets = torch.arange(tensor.shape[0], device=device) * len(id_to_word)\n",
    "    model.eval()\n",
    "\n",
    "    acc = torch.zeros(tensor.shape[0], device=device)\n",
    "    for i in range(block_size, tensor.shape[1] - block_size):\n",
    "        start_point = i - block_size\n",
    "        # print('Getting probabilities')\n",
    "        probabilities = predict_probabilities2(model, tensor[:, start_point:i])\n",
    "        # print(probabilities)\n",
    "        # print('Getting ids')\n",
    "        expected_ids = tensor[:, i]\n",
    "\n",
    "        print(f'Round {i}    ', end='\\r')\n",
    "        # print()\n",
    "        # print('Summing offsets')\n",
    "        summed_offsets = expected_ids + take_offsets\n",
    "        # print('Indexing probabilities')\n",
    "        probs = probabilities.take(summed_offsets)\n",
    "        # print('Converting probs to tensor')\n",
    "        # probs = torch.as_tensor(probs)\n",
    "        # print('Adding to total')\n",
    "        acc += probs\n",
    "    return acc / tensor.shape[1]\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "# print(f\"Average probability of author 11 being author {12}: {get_prob_avg(load_or_train(11), get_author_tensor(12)[10]):.4f}\")\n",
    "print(f\"Average probability of author 12 being author {12}: {get_prob_avg2(load_or_train(12), get_author_tensor(12)[10].unsqueeze(0))}\")\n",
    "# print(f\"Average probability of author 13 being author {12}: {get_prob_avg(load_or_train(13), get_author_tensor(12)[10]):.4f}\")\n",
    "end = time.time()\n",
    "print('Time diff: ', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3437c6-100b-48ca-9f29-d969f94b61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache = {}\n",
    "def get_model_cached(author_id):\n",
    "    if not author_id in model_cache:\n",
    "        model_cache[author_id] = load_or_train(author_id)\n",
    "    return model_cache[author_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3dc2dd4-7aa8-4771-91dd-0b3129ff825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_author(tensor):\n",
    "    best_probability = 0\n",
    "    best_author = None\n",
    "    for author_id in valid_authors:\n",
    "        probability = get_prob_avg(get_model_cached(author_id), tensor)\n",
    "        print(f'Chance author {author_id} {id_to_author[author_id]} wrote this tensor: {probability * 100:.2f}')\n",
    "        if probability > best_probability:\n",
    "            best_probability = probability\n",
    "            best_author = author_id\n",
    "    return best_author\n",
    "\n",
    "def get_authors(sample_text):\n",
    "    sample_bestprobability = [0] * sample_text.shape[0]\n",
    "    sample_bestauthor = [None] * sample_text.shape[0]\n",
    "    # count = 0\n",
    "    # limit = 2\n",
    "    for author_id in valid_authors:\n",
    "        # count += 1\n",
    "        # if count > limit:\n",
    "        #     break\n",
    "        sample_probability = get_prob_avg2(get_model_cached(author_id), sample_text)\n",
    "        print('Author {author_id} {id_to_author[author_id]} done.')\n",
    "        # for sample_index, probability in enumerate(sample_probability):\n",
    "        #     print(f'Chance author {author_id} {id_to_author[author_id]} wrote tensor {sample_index}: {probability * 100:.2f}')\n",
    "        for i in range(len(sample_bestprobability)):\n",
    "            probability = sample_probability[i]\n",
    "            best_probability = sample_bestprobability[i]\n",
    "            if probability > best_probability:\n",
    "                sample_bestprobability[i] = probability\n",
    "                sample_bestauthor[i] = author_id\n",
    "    return sample_bestauthor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab5bcb87-bdcc-4c5c-ab25-40b9b4330639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1789), (1789, 3578), (3578, 5367), (5367, 7156), (7156, 8945), (8945, 10734)]\n",
      "torch.Size([10736, 1001])\n",
      "[torch.Size([1789]), torch.Size([1789]), torch.Size([1789]), torch.Size([1789]), torch.Size([1789]), torch.Size([1789])]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "segment_size = math.floor(test_data.shape[0] / 6)\n",
    "segment_indices = []\n",
    "for segment_index in range(6):\n",
    "    start = segment_size * segment_index\n",
    "    end = segment_size * (segment_index + 1)\n",
    "    segment_indices += [(start, end)]\n",
    "    \n",
    "print(segment_indices)\n",
    "print(test_data.shape)\n",
    "\n",
    "expected_segments = []\n",
    "for start, end in segment_indices:\n",
    "    expected_segments.append(test_data[start:end, -1])\n",
    "print([t.shape for t in expected_segments])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd86c1a1-49d1-4083-a049-d5a07b098687",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segments = [None, None, None, None, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b6439-28f0-4850-80f0-92a2e18b4a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 617    \r"
     ]
    }
   ],
   "source": [
    "segment = 0\n",
    "start, end = segment_indices[segment]\n",
    "expected = expected_segments[segment]\n",
    "\n",
    "sample_textauthor = test_data[start:end]\n",
    "sample_text = sample_textauthor[:, :-1]\n",
    "predictions = get_authors(sample_text)\n",
    "# print('Expected is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in sample_authorid))\n",
    "# print('Observed is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in predictions))\n",
    "predicted_segments[segment] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876f449-ee61-4445-8c77-a14a29f9c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 1\n",
    "start, end = segment_indices[segment]\n",
    "expected = expected_segments[segment]\n",
    "\n",
    "sample_textauthor = test_data[start:end]\n",
    "sample_text = sample_textauthor[:, :-1]\n",
    "predictions = get_authors(sample_text)\n",
    "# print('Expected is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in sample_authorid))\n",
    "# print('Observed is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in predictions))\n",
    "predicted_segments[segment] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139feb53-7f15-4ad1-8610-41deb1674e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 2\n",
    "start, end = segment_indices[segment]\n",
    "expected = expected_segments[segment]\n",
    "\n",
    "sample_textauthor = test_data[start:end]\n",
    "sample_text = sample_textauthor[:, :-1]\n",
    "predictions = get_authors(sample_text)\n",
    "# print('Expected is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in sample_authorid))\n",
    "# print('Observed is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in predictions))\n",
    "predicted_segments[segment] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90257e-47de-49ec-8aac-0c2ae7aa6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 3\n",
    "start, end = segment_indices[segment]\n",
    "expected = expected_segments[segment]\n",
    "\n",
    "sample_textauthor = test_data[start:end]\n",
    "sample_text = sample_textauthor[:, :-1]\n",
    "predictions = get_authors(sample_text)\n",
    "# print('Expected is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in sample_authorid))\n",
    "# print('Observed is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in predictions))\n",
    "predicted_segments[segment] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7595c7-e045-44cc-8724-905f3da2fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 4\n",
    "start, end = segment_indices[segment]\n",
    "expected = expected_segments[segment]\n",
    "\n",
    "sample_textauthor = test_data[start:end]\n",
    "sample_text = sample_textauthor[:, :-1]\n",
    "predictions = get_authors(sample_text)\n",
    "# print('Expected is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in sample_authorid))\n",
    "# print('Observed is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in predictions))\n",
    "predicted_segments[segment] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73cabaf-551b-4b18-adc7-c6aeebb90c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 5\n",
    "start, end = segment_indices[segment]\n",
    "expected = expected_segments[segment]\n",
    "\n",
    "sample_textauthor = test_data[start:end]\n",
    "sample_text = sample_textauthor[:, :-1]\n",
    "predictions = get_authors(sample_text)\n",
    "# print('Expected is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in sample_authorid))\n",
    "# print('Observed is\\n\\t', '\\n\\t'.join(f'{author_id}: {id_to_author[author_id]}' for author_id in predictions))\n",
    "predicted_segments[segment] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2015108b-1183-4c96-ab01-91f124297890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 19, 12, 21, 27, 27, 14, 14, 14, 26, 48, 43, 8, 8, 36, 14, 19, 43, 20, 3, 37, 44, 37, 17, 45, 32, 25, 45, 23, 33, 8, 14, 9, 39, 26, 8, 12, 39, 18, 18, 18, 41, 21, 8, 9, 25, 18, 32, 33, 26, 32, 44, 10, 19, 8, 50, 37, 35, 37, 8, 1, 23, 36, 37, 39, 20, 1, 26, 10, 37, 8, 8, 15, 32, 39, 48, 23, 26, 45, 17, 37, 15, 36, 16, 46, 15, 18, 45, 30, 34, 39, 10, 9, 8, 10, 14, 26, 19, 1, 26]\n",
      "[18, 19, 12, 21, 27, 27, 14, 14, 14, 26, 48, 43, 8, 8, 36, 14, 19, 43, 20, 3, 37, 44, 37, 17, 45, 32, 25, 45, 23, 33, 8, 14, 9, 39, 26, 8, 12, 39, 18, 18, 18, 8, 21, 8, 9, 25, 18, 32, 33, 26, 32, 44, 10, 19, 8, 50, 37, 8, 37, 8, 1, 23, 36, 37, 39, 20, 1, 26, 10, 37, 8, 8, 15, 32, 39, 48, 23, 26, 45, 17, 37, 15, 36, 16, 46, 15, 18, 45, 30, 34, 39, 10, 9, 8, 10, 14, 26, 19, 1, 26, 8, 8, 39, 1, 33, 4, 4, 8, 21, 45, 30, 6, 4, 26, 9, 26, 19, 33, 42, 45, 45, 8, 9, 26, 14, 19, 34, 14, 46, 45, 21, 35, 26, 25, 26, 48, 8, 36, 26, 38, 12, 1, 8, 33, 9, 21, 44, 25, 26, 4, 33, 45, 16, 35, 17, 21, 14, 33, 32, 18, 2, 42, 10, 11, 8, 39, 14, 45, 4, 45, 38, 16, 21, 44, 15, 9, 26, 26, 1, 18, 30, 40, 4, 33, 37, 15, 9, 30, 37, 19, 14, 4, 37, 25, 30, 8, 8, 26, 8, 8, 18, 25, 8, 18, 8, 25, 20, 8, 48, 19, 14, 4, 18, 25, 8, 4, 48, 13, 26, 26, 23, 19, 8, 8, 12, 9, 18, 11, 6, 48, 15, 11, 48, 9, 33, 29, 14, 28, 37, 4, 14, 4, 40, 8, 26, 41, 29, 26, 26, 50, 25, 4, 24, 29, 32, 45, 18, 18, 16, 43, 8, 29, 1, 26, 8, 19, 26, 33, 26, 9, 19, 8, 37, 25, 35, 16, 26, 10, 4, 29, 8, 8, 8, 26, 30, 14, 8, 28, 39, 39, 30, 21, 20, 44, 33, 15, 26, 15, 8, 43, 9, 10, 8, 48, 17, 33, 42, 35, 14, 45, 10, 4, 33, 37, 32, 35, 45, 14, 8, 8, 26, 26, 24, 33, 45, 45, 22, 10, 9, 37, 21, 45, 21, 21, 29, 41, 26, 23, 45, 9, 9, 25, 37, 39, 41, 9, 26, 28, 21, 26, 39, 41, 33, 6, 14, 10, 20, 19, 20, 45, 25, 18, 35, 15, 38, 4, 23, 14, 15, 26, 8, 37, 34, 20, 33, 4, 8, 8, 39, 29, 37, 48, 38, 14, 14, 34, 45, 26, 33, 14, 33, 25, 48, 26, 45, 8, 12, 44, 28, 8, 8, 21, 33, 26, 21, 9, 43, 45, 43, 16, 8, 20, 13, 44, 15, 8, 39, 8, 28, 39, 19, 14, 10, 15, 12, 19, 14, 29, 26, 14, 26, 30, 23, 48, 15, 45, 8, 38, 14, 16, 18, 14, 37, 36, 16, 8, 48, 12, 38, 14, 21, 11, 4, 15, 48, 22, 25, 14, 45, 18, 26, 39, 37, 8, 43, 21, 8, 20, 8, 26, 14, 8, 37, 19, 37, 4, 8, 33, 37, 18, 18, 14, 8, 14, 21, 43, 45, 9, 8, 19, 42, 8, 37, 16, 18, 15, 15, 37, 20, 48, 9, 42, 8, 8, 8, 32, 37, 45, 48, 13, 50, 50, 32, 39, 14, 4, 24, 28, 8, 14, 4, 35, 35, 4, 33, 26, 8, 4, 37, 39, 33, 14, 45, 1, 33, 14, 39, 34, 45, 26, 8, 18, 15, 48, 21, 38, 8, 48, 26, 9, 34, 8, 20, 39, 14, 25, 26, 32, 37, 9, 33, 37, 26, 4, 8, 29, 8, 8, 48, 8, 18, 26, 8, 36, 45, 30, 2, 26, 22, 6, 37, 39, 30, 8, 45, 26, 33, 14, 8, 48, 8, 24, 14, 44, 26, 21, 19, 19, 21, 25, 15, 8, 41, 42, 40, 21, 2, 48, 39, 50, 29, 14, 21, 8, 8, 26, 48, 24, 8, 39, 14, 36, 8, 26, 39, 22, 25, 26, 8, 48, 8, 14, 33, 17, 35, 8, 26, 39, 4, 4, 17, 8, 48, 43, 21, 14, 35, 35, 18, 44, 26, 43, 15, 45, 42, 41, 30, 8, 26, 8, 29, 9, 38, 26, 40, 26, 33, 8, 41, 19, 41, 14, 11, 8, 8, 8, 21, 2, 19, 37, 45, 37, 2, 4, 26, 26, 39, 19, 33, 45, 14, 8, 8, 50, 39, 29, 50, 26, 10, 23, 14, 4, 50, 8, 45, 42, 21, 32, 33, 26, 9, 23, 8, 21, 48, 21, 50, 33, 8, 8, 37, 41, 38, 26, 26, 1, 8, 37, 33, 19, 36, 4, 14, 1, 19, 24, 46, 19, 17, 13, 25, 22, 15, 48, 8, 8, 11, 12, 39, 33, 37, 38, 33, 21, 33, 21, 44, 19, 48, 4, 15, 8, 8, 26, 42, 26, 37, 40, 21, 50, 50, 8, 38, 45, 39, 36, 45, 15, 26, 39, 45, 21, 15, 26, 12, 37, 17, 1, 45, 8, 45, 48, 26, 30, 30, 48, 8, 22, 8, 45, 39, 14, 10, 9, 19, 37, 45, 37, 8, 19, 26, 30, 45, 20, 8, 32, 15, 43, 37, 33, 30, 14, 43, 8, 26, 21, 39, 37, 9, 45, 37, 37, 21, 1, 39, 14, 8, 38, 26, 21, 12, 48, 30, 44, 48, 37, 34, 25, 15, 45, 28, 26, 19, 8, 8, 39, 9, 43, 10, 37, 14, 4, 42, 23, 30, 34, 32, 45, 23, 22, 8, 14, 32, 4, 8, 8, 13, 8, 26, 37, 43, 26, 17, 9, 43, 9, 21, 3, 21, 39, 48, 34, 17, 45, 45, 25, 45, 25, 27, 23, 26, 32, 26, 14, 21, 45, 36, 19, 48, 12, 40, 9, 33, 8, 15, 19, 4, 15, 26, 15, 23, 41, 25, 18, 38, 25, 50, 41, 19, 45, 18, 39, 26, 8, 10, 48, 14, 9, 9, 9, 17, 26, 1, 8, 15, 19, 48, 34, 35, 45, 26, 14, 8, 44, 9, 14, 13, 8, 37, 14, 36, 14, 18, 20, 4, 26, 14, 43, 8, 15, 10, 8, 8, 26, 45, 48, 26, 50, 26, 10, 13, 29, 8, 38, 8, 33, 45, 32, 19, 13, 8, 19, 50, 14, 43, 45, 15, 15, 45, 21, 33, 17, 33, 33, 15, 41, 11, 8, 39, 10, 23, 21, 8, 26, 30, 38, 10, 39, 27, 11, 8, 41, 19, 39, 9, 12, 48, 44, 8, 37, 9, 12, 32, 39, 45, 19, 6, 34, 13, 3, 41, 33, 21, 33, 43, 14, 42, 8, 23, 1, 21, 26, 45, 9, 9, 21, 36, 10, 45, 8, 25, 45, 35, 26, 33, 45, 8, 23, 1, 8, 8, 40, 18, 4, 45, 37, 36, 14, 8, 6, 26, 37, 26, 26, 14, 39, 37, 41, 43, 48, 14, 34, 18, 11, 14, 38, 48, 14, 37, 8, 44, 37, 39, 44, 37, 15, 9, 6, 28, 9, 24, 14, 4, 8, 8, 43, 9, 45, 30, 8, 32, 34, 30, 25, 46, 37, 4, 13, 37, 21, 1, 50, 19, 8, 14, 26, 8, 1, 38, 14, 15, 11, 33, 14, 37, 33, 9, 34, 2, 42, 43, 8, 19, 19, 18, 45, 14, 15, 21, 50, 27, 46, 41, 8, 26, 26, 43, 39, 1, 26, 14, 42, 50, 20, 36, 39, 12, 8, 6, 39, 39, 8, 4, 40, 8, 42, 50, 45, 8, 38, 8, 35, 39, 26, 8, 45, 33, 29, 43, 14, 50, 32, 12, 9, 39, 8, 28, 21, 46, 14, 43, 14, 38, 25, 21, 12, 37, 23, 26, 48, 42, 4, 50, 13, 21, 8, 33, 17, 21, 18, 48, 34, 8, 41, 8, 38, 37, 21, 25, 39, 19, 26, 21, 42, 25, 26, 42, 8, 39, 26, 9, 8, 8, 21, 33, 26, 39, 9, 8, 33, 20, 37, 14, 42, 4, 26, 9, 50, 14, 8, 21, 33, 22, 26, 28, 39, 40, 26, 2, 26, 18, 48, 42, 8, 24, 15, 39, 8, 42, 26, 8, 23, 1, 39, 9, 1, 14, 12, 42, 4, 39, 35, 37, 8, 14, 21, 14, 1, 26, 25, 33, 8, 8, 20, 42, 19, 14, 6, 34, 36, 4, 8, 11, 11, 4, 26, 18, 10, 8, 39, 26, 8, 45, 41, 45, 37, 39, 29, 33, 30, 10, 8, 48, 35, 28, 1, 39, 26, 17, 36, 43, 32, 8, 25, 50, 8, 8, 21, 14, 38, 10, 8, 48, 1, 4, 23, 21, 15, 8, 43, 37, 28, 19, 10, 15, 44, 48, 3, 4, 37, 8, 29, 33, 45, 19, 9, 44, 45, 8, 18, 8, 11, 33, 39, 43, 19, 36, 8, 9, 4, 39, 14, 28, 38, 9, 8, 37, 33, 3, 11, 26, 11, 21, 14, 45, 43, 26, 21, 18, 33, 37, 29, 39, 8, 45, 15, 13, 43, 33, 23, 34, 19, 26, 8, 9, 19, 28, 48, 38, 26, 22, 19, 37, 39, 18, 10, 24, 28, 21, 8, 26, 9, 10, 4, 4, 40, 30, 26, 15, 18, 39, 8, 8, 39, 19, 25, 8, 40, 26, 8, 8, 43, 50, 26, 43, 39, 26, 1, 32, 39, 37, 45, 25, 37, 22, 26, 8, 33, 33, 21, 48, 15, 8, 37, 2, 4, 16, 8, 26, 26, 48, 48, 38, 15, 32, 39, 8, 43, 43, 26, 1, 35, 39, 8, 45, 26, 14, 26, 15, 48, 30, 19, 36, 14, 26, 26, 8, 14, 8, 39, 8, 15, 50, 48, 33, 8, 8, 14, 13, 33, 26, 8, 8, 43, 26, 21, 8, 4, 17, 43, 4, 26, 45, 48, 40, 37, 45, 4, 13, 4, 8, 6, 50, 2, 19, 27, 8, 15, 26, 12, 42, 35, 43, 22, 43, 30, 8, 38, 4, 50, 8, 25, 9, 8, 30, 30, 14, 4, 14, 18, 8, 27, 18, 37, 38, 23, 8, 25, 26, 36, 8, 50, 43, 19, 39, 20, 45, 12, 15, 43, 15, 26, 8, 21, 3, 19, 45, 14, 8, 4, 9, 32, 20, 19, 37, 46, 48, 8, 45, 26, 33, 33, 45, 25, 8, 26, 4, 33, 21, 26, 21, 37, 19, 26, 4, 8, 34, 22, 2, 21, 26, 9, 26, 9, 8, 12, 17, 27, 4, 14, 26, 26, 26, 8, 38, 8, 37, 8, 42, 41, 8, 26, 39, 18, 20, 14, 14, 1, 33, 1, 8, 37, 13, 35, 1, 22, 8, 26, 26, 14, 8, 4, 1, 48, 37, 14, 37, 14, 13, 35, 8, 4, 28, 15, 38, 4, 27, 23, 8, 48, 21, 39, 37, 39, 26, 41, 8, 37, 26, 48, 12, 14, 25, 39, 13, 25, 15, 14, 33, 45, 35, 12, 21, 48, 42, 26, 43, 38, 43, 15, 14, 50, 38, 8, 11, 16, 26, 21, 39, 28, 33, 10, 25, 39, 30, 2, 33, 1, 48, 2, 17, 8, 1, 14, 14, 48, 36, 48, 17, 8, 10, 15, 29, 37, 37, 8, 14, 16, 39, 8, 25, 10, 45, 32, 48, 1, 33, 18, 38]\n"
     ]
    }
   ],
   "source": [
    "# print(predicted_segments)\n",
    "print(predicted_segments[0])\n",
    "# print(expected_segments[0].tolist())\n",
    "# sample_predictedauthorid = torch.tensor(predictions)\n",
    "# accuracy = torch.sum(sample_predictedauthorid == sample_authorid) / sample_authorid.shape[0]\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c95d3-882a-4165-a296-06bbfbc3ea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
